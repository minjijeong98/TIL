# 수능형 문제 풀이 모델 생성 프로젝트

## 프로젝트 개요
- Task: 수능 특화 언어모델 만들기
    - '한국어'와 '시험'이라는 주제에 맞춰서 작은 모델들로 수능 시험을 풀어보기
- Metric: accuracy
    - 모델이 맞춘 문제 수 / 전체 문제 수
- 기간: 24.11.11. ~ 24.11.28 (3주)
- 규정
    - 외부 데이터
        - 수능(전체 영역), 평가원 모의고사, 수능 관련 문제집(EBS/사설 교재 등) 등 수능과 관련된 데이터는 일절 사용이 금지
        - 그 외 저작권상 활용 가능한 모든 외부 데이터는 사용을 허용
    - 데이터 증강
        - 제공된 학습 데이터와 저작권상 활용 가능한 외부 데이터를 기반으로 데이터 증강이 가능
        - AI를 활용한 유료 API 사용을 허용
        - 수능 관련 데이터를 시드 데이터로 활용하거나 이를 기반으로 사람이 직접 데이터를 제작하는 것은 불가
    - 기학습 가중치
        - Ko-MMLU, Multilingual MMLU, KLUE-MRC, 수능 데이터로 학습된 기학습 가중치 사용은 금지
- 데이터 구성
    - 수능형 문제: 수능의 국어, 사회 영역(윤리, 정치, 사회)과 비슷한 문제
    - KMMLU (Korean History), MMMLU (HighSchool 데이터 중 역사, 경제, 정치, 지리, 심리)
    - KLUE MRC (경제, 교육산업, 국제, 부동산, 사회, 생활, 책마을)

## 접근 방법
### 내가 이해한 프로젝트
- data centric: 결국 양질의 데이터로 LLM 파인튜닝하고, 데이터를 이용해 원하는 품질 낸다는 점에서 이번 프로젝트는 진정한 의미의 data-centric이라고 생각한다 (이전 프로젝트보다..). 몇몇 주요 오픈소스 sLLM들의 fine-tuning 방법을 익히고 난 이후 시점부터는 데이터 핸들링의 문제일 것. 
- RAG: 한정된 데이터 기반으로 수능 문제에 대해 정확한 답 도출해내라는건 무리일 것. 특히 수능 데이터 직접 사용할 수 없는 상황이므로. 따라서 데이터 모아 DB 구축하고 이를 활용해 RAG 적용하는게 성능 향상에 중요할 것으로 생각됨

### 주요 단계별 접근
1. EDA: 각 문제의 분야 및 유형 분석 필요 - "어떤 문제를 풀어야 하는가?"
    1. 분야
        1. 국어: 수능 기준으로 보면.. 화작, 문법, 문학, 비문학 정도일 것 같다. 구체적으로 어떤 유형의 국어 문제를 푸는지 확인 필요
        2. 사회: 상당히 광범위한 사회 데이터 언급됨 (윤리, 정치, 사회, 역사, 경제, 지리, 심리, 교육, 국제, 생활 등). 이들 문제의 공통점/차이점 있는지 확인 필요
    2. 문제 유형
        1. 기본적으로 5지선다형이지만, '보기' 바탕으로 다중 선택하는 항목도 섞여있는 듯 함
        2. 어떤 방식으로 문제 구성되어있는지 확인 필요.
    - 이들 내용 정량적, 수치적으로 확인 필요
        - 초기 데이터 탐색 단계에서 각 학습데이터에 대해 수동 레이블링 진행해볼 생각
        - 이후 합성데이터 생성 시 어떤 데이터를 어떻게 생성할건지 비율/개수 결정해야 할텐데, 이렇게 파악해둔 내용이 도움 될 것
        - 한편, 대량의 합성 데이터에 대해 분야/유형별 분포 및 퀄리티 확인 위해 knowledge classification 모델 만들 수 있을 것. 이 때 모델 SFT 위한 데이터셋으로 이 수동 레이블링 데이터셋 사용 가능할 것
    - 베이스라인 모델로 학습 돌려보고, 이 분석 기준 바탕으로 현재 모델이 잘/못하는 케이스 탐지하기
2. 모델 선정 (sLLM)
    1. 최종 정답 인퍼런스 모델
        1. 주요 모델 및 fine-tuning 방법 탐색, 확인
        2. 최종 성능에 중요한 영향 미칠테니, 이 모델 잘 선정하는게 중요할 것
        3. 국어/사회가 푸는 문제 유형 다르다면 따로 모델 적용해도 괜찮을 것
            1. 국어: 문맥 속에서 답 최대한 잘 추론하도록 학습
            2. 사회: RAG 등.
    2. 합성 데이터 생성 모델
        1. 문제는 잘 만들어야 하니, 유료 모델 사용도 고려해볼 수 있음
    3. 기타 분류 및 평가 모델
        1. 문제 카테고리/유형 분류기
        2. 데이터 품질 판별기
        3. sLLM 잘 튜닝하거나 few-shot으로 처리. 힘 많이 쏟을 건 아닐 듯.
3. 평가 metric 탐색
    1. QA: accuracy (정답 얼마나 잘 맞추는지)
    2. hallucination
        1. faithful test (주어진 지문에 근거 존재하는지. 정확한 근거 기반으로 답변하는지)
        2. self-consistency (일관된 답변 하는지)
        3. 이 부분 체크한다면, 이걸 판별하는 모델 만들어야 할 듯
4. 외부 데이터 탐색
    1. 고등학교 교과서 데이터, LEET 기출문제(국어), 사회 문제들 풀기 위한 다양한 데이터 쌓아야 할 듯 (분야 다양..)
        1. 이 데이터 어디에 쓸 것?
            1. 이 데이터들 기반으로 합성 데이터 생성 (문제, 정답 데이터 생성)
            2. RAG에서 retrieval을 위한 DB 구축
    2. LLM 평가용 데이터셋 탐색
        1. 각 목적별로 평가셋 탐색 
            1. 수능 문제 잘 푸는지: Ko-MMLU, Multilingual MMLU, KLUE-MRC 등
            2. 문제 카테고리 잘 분류하는지: Topic classification 관련 데이터 등
        2. 평가 지표도 함께 탐색
    3. (추가) 코드 데이터 추가? 수학 문제 추가?
        1. 강의 듣다보니, 코드를 학습시키면 논리적으로 사고하고 간결한 답변 생성에 도움 된다길래. 우리 task에서도 그럴 수 있지 않을까?
        2. 아니면 수능 문제풀이라는 형식 유지 위해, 수학 문제 푸는 예시 좀 추가해서 reasoning 능력 강화할 수 있지 않을까?
5. RAG 구성 및 적용
    - 우선 앞서 선정된 모델 결과 보면서, retrieve가 필요한지, RAG 사용한다면 성능 개선될지 판단하기 (RAG 적용하면 리소스 많이 들어갈거라... 필요하다는 판단 이후 진행하는게 나을 듯)
    - 일반 RAG, Graph RAG, TAG 등
        - TAG 쓴다면, RDBMS에 table로 데이터 저장해두고 -> 파인튜닝된 LLM 주제 분류기로 주제 탐지해서 -> 해당 주제 내에서 관련 문서 찾아 추론 진행하면 되지 않을까?
    - retrieval: 벡터DB 사용해보면 어떨까?
        - 지난번 Milvus 사용했는데, 다양한 벡터DB 존재하니 사용해보면 좋을 것
        - decode-based embedding model로 임베딩 생성해보면 어떨까
    - reader: LLM 모델 잘 fine-tuning해서 성능 향상시키기
        - 꼭 fine-tuning 해야 할까? CoT나 prompt engineering 기술로는 해결 안되나? fine-tuning하면 성능/시간 얼마나 개선될까?
        - 각 보기가 맞는/틀린 이유도 같이 출력하도록 하면 성능 오를까? (CoT와 유사한 접근)
6. 데모 페이지 제작
    - tool: streamlit, gradio 등 (streamlit이 더 예쁘긴 해..)
    - 목적: 모델의 작동 방식 시각화 및 이해 증진
    - 흐름: validation set에서 랜덤으로 문제 가져오고 -> 모델이 어떻게 문제 푸는지 중간과정 보여주고 (ex. RAG 사용한다면 어떤 데이터를 참고 데이터로 가져왔는지) -> 모델이 추론한 정답 보여주고 -> 맞았는지 틀렸는지 보여주기 (validation set에서 데이터 가져오므로 정답 데이터 있으니까)

### Further Question
- 데이터셋 버전 관리 방법 어떻게 할까?
    - 데이터셋 버전 다양하게 나올텐데, 미리 versioning 및 관리 방법 정해두는게 나을지?
    - 아니면 해보면서 결정?
- 벡터DB 서버 간 공유 가능? 혹은 huggingface 통해서라도?
    - 데이터 parquet 또는 csv 등으로 공유해서, 각자 서버에 구워야 하나?
- task 관련 기존 접근방법 조사
    - SAT 문제 풀이 task는 LLM 능력 테스트한다면서 많이 수행되온걸로 앎
    - 어떤 방식으로 접근하고, 뭐가 성능 좋다고 하는지 확인해보기
- LLM의 주요 bias: Length, Order
    - 문제 관련 지문 길이 어느 정도일때 최적? 잘라야 하나?
    - 보기의 선지 순서 바뀌어도 일관된 성능 보이나?
- 속도 개선 방법
    - 한 번 학습에 3시간은 너무 긺. 지금은 바닐라 모델이니까, 최대한 속도 개선 방법 찾아봐야 함.

