# 프로젝트 접근 전략 Ideation

> [!TIP] 목적
> Data-centric한 다양한 접근방법이 존재한다. 어떤 방법을 어떤 순서로 적용할 지 정리하기 위해 접근 방법들을 구체적으로 나열해본다. 다음주 월요일까지 이 흐름에 따라 시도해볼 예정

## 1. 초기 데이터 분석 및 베이스라인 설정

### 1. EDA (Exploratory Data Analysis)
- train 데이터의 전체적 특성 파악
- `klue/bert-base` tokenizer 사용한 토큰 분석
    - 기사제목 길이 분포, unknown 토큰 비율, 노이즈에 따른 토큰화 영향 분석
- Label 분포 확인
    - 라벨 분포 및 불균형 시각화
    - true label mapping 대략 확인
- 노이즈 데이터와 정상 데이터 분리 분석
    - 노이즈 종류/분포/정도 확인 -> 노이즈 처리 전략 수립

### 2. 데이터 품질 메트릭 정의 및 측정
- 이후 증강된 데이터 등에도 일괄적으로 적용할 수 있는 평가 기준 수립
- 데이터의 전반적 품질 평가
    - 데이터의 완전성, 일관성, 정확성 등 (결측치 없고, 라벨이 7개 카테고리 중 하나고, 기사 제목의 길이 적절한지 등)
    - [Great Expectations 라이브러리](https://otzslayer.github.io/mlops/2022/10/19/great-expectations-from-scratch.html ) 활용 고려해보기
- 라벨 품질 평가
    - cleanlab을 사용한 잠재적 라벨 오류 탐지
    - LLM(Llama, Gemma, Solar, Mistral 등)을 사용한 IAA 계산
        - 재할당된 라벨들의 일관성을 LLM을 통해 검증
        - LLM에게 같은 클러스터 내의 여러 기사 제목을 제공하고, 이들이 같은 주제로 분류될 수 있는지 확인 요청 (이진 분류)
        - 불일치가 발견된 경우, 해당 샘플들을 재검토 대상으로 표시
- 신뢰할 수 있는 Validation set 구성
    - 정상 데이터(label 정상, 노이즈 없음) 따로 추출해 validation으로 정해두기 
        - 앙상블 접근법: 여러 LLM 모델을 사용하여 일부 데이터에 대해 라벨링 수행, 높은 일치도를 보이는 샘플만 선택
- 성능 평가 기준점
    - validation set, test set에 대한 평가 기준: accuracy, f1-score(macro)
    - leaderboard 기준이기도 함

### 3. 데이터 버전 관리
- [Data Version Control (DVC)](https://lsjsj92.tistory.com/573) 활용해볼까?



## 2. 데이터 클리닝 & 필터링

노이즈와 라벨링 오류가 동시에 발생하는 경우 없다고 공지되었으니, 이들을 별개로 나누어 처리함

### 1. 노이즈 처리
- 노이즈 데이터 탐지 (PyOD 등?)
- 노이즈 정도에 따른 데이터 분류
- 심한 노이즈 데이터 삭제
- 경미한 노이즈 데이터 복구 시도: back translation 활용
    - 어느 정도 범위의 노이즈 어떻게 처리할 지 실험 및 결정 필요
- 처리된 데이터 대해 LLM 기반 라벨 검증

2. 라벨 개선
- 불확실한 라벨 식별: cleanlab 활용
- 임베딩 기반 클러스터링 및 문장 유사도 계산을 통해 라벨 재부여?
    - 노이즈 처리 완료된 데이터는 라벨 정확하니, 이들의 기사제목 임베딩 벡터와 클러스터 정보 이용해서, 유사도 높은 기사는 동일한 라벨 갖도록 분류
- LLM에게 기사제목 보고 카테고리 생성하라고 시킨 뒤, 이 카테고리의 코사인 유사도로 라벨 재부여
    - 상위 10% 불확실 샘플에 대해 3개 이상의 LLM으로 재라벨링 -> 다수결 원칙 적용해볼수도?
- LLM 앙상블을 통한 라벨 품질 평가:
    - LLM에게 기사 제목만 제공하고, 해당 제목이 어떤 주제에 속하는지 자유롭게 분류하도록 함
    - LLM의 분류 결과와 기존 라벨을 비교하여 일치 여부 확인 (T/F)
    - 주의사항
        - LLM에게 기존 라벨 카테고리나 개수에 대한 정보를 제공하지 않아야 함
        - 여러 LLM을 사용하여 앙상블 평가 수행

## 3. 데이터 증강

### 1. rule-based 접근
- EDA, UDA 구현
- 증강 비율 실험 (원본 대비 10%, 50%, 100%, 150%, 200%, 300%)

### 2. example interpolation
- mixup 구현 (MSDSA 등)
- 증강 비율 실험 (원본 대비 10%, 50%, 100%, 150%, 200%, 300%)

### 3. Model-based 접근
- Back Translation 구현
    - option 1: 번역기 모델
    - option 2: 중간 번역어
- 증강 비율 실험 (원본 대비 10%, 50%, 100%, 150%, 200%, 300%)
- 얼마나 유사한지 확인: 각 방법별 BLEU 스코어 측정 및 비교 


## 4. 합성 데이터 생성

### 1. LLM 기반 데이터 생성
- 여러 LLM 모델 대해 시도 (Llama, Gemma, Solar, Mistral 등)
- 라벨 정보를 이용할 수 없으니, 라벨별 클러스터 나눠놓고 유사한 방향으로 뉴스 기사를 생성하도록 시키자
    - 각 뉴스 기사에 대한 임베딩 벡터 생성하고 -> k-means, DBSCAN 등으로 클러스터링
        - 클러스터 수 지정할 수 있다는 점에서 k-means가 나을 듯?
    - 각 클러스터의 특성 LLM에게 학습시키기
    - LLM에게 각 클러스터와 유사한 기사 제목 생성하도록 요청
        - Prompt engineering + few-shot
        - 여러 프롬프트 시도
        - few-shot 시도: 7개 카테고리별로 예시 다르게 설정.
            - 각 카테고리별로 고정된 예시 부여
            - 약 20개 예시 각각 만들어두고, 랜덤하게 few-shot 바꿔가며 생성 (-> 다양성 확보 가능)
    - 생성된 제목들을 기존 데이터셋과 비교해서 유사도 분석
        - 전반적으로 유사도 높게 나와야 하지만, 너무 유사도 높으면 삭제

### 2. 생성 데이터 품질 검증
- STS 모델, SBERT 모델 등 사용해 문장 유사도 검사
- 중복 및 과도하게 유사한 샘플 필터링 (임계값 설정 필요)


## 5. 실험 설계 및 평가
### 1. 점진적 개선 평가
- 각 단계별 모델 성능 변화 추적
- wandb를 사용한 실험 로깅 및 시각화
- 각 과정에서 추가되는 데이터의 세부 성능 평가
    - Bert score, 
    - Great Expectations 점수, 
    - cosine similarity score 등

### 2. 최종 평가
- test score 평가 (macro f1-score, accuracy)

### 3. 각 실험에서 기록할 내용
```json
{
  "experiment_id": "",
  "date": "",
  "purpose": "",
  "changes": [],
  "results": {
    "accuracy": 0,
    "f1_score": 0,
    "roc_auc": 0
  },
  "notes": ""
}
```

---
## 본 프로젝트에서 LLM의 역할
- evaluator, annotator: 품질 평가, 일관성 검증
- generator: 클러스터의 특성 반영하는 기사 제목 생성
- 직접적으로 라벨을 할당할지는 물음표..